<!DOCTYPE HTML>
<html>

<head>
  <!-- Google analytics tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WRYQ3GG5Y8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-STGLQW4BJX');

    function toggleExcerpt(elementId) {
      const excerptDiv = document.getElementById(elementId);
      if (excerptDiv.style.display === "none") {
        excerptDiv.style.display = "block";
      } else {
        excerptDiv.style.display = "none";
      }
    }
  </script>

  <link rel="icon" type="image/png" href="images/favicon.png" />

  <!-- Title -->
  <title>Kay - Physical Intelligence</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=1000">

  <!-- Isotope JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.13.2/jquery-ui.min.js"></script>
  <script src="https://unpkg.com/isotope-layout@3/dist/isotope.pkgd.min.js"></script>

  <!-- Custom Style -->
  <link rel="stylesheet" href="style.css">

  <!-- Google Font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap"
    rel="stylesheet">
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Asap:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap');
  </style>
</head>

<body id="body">

  <div id="main">
    <div id="intro">
      <div id="intro-text">
        <h1>Kay - Liyiming Ke</h1>
        <p>
          Hi 👋 I work at <a href="https://www.physicalintelligence.company/">Physical Intelligence</a>
          researching on Machine Learning for Robot Manipulation.
          During my PhD at University of Washington, I built a chopsticks-welding robot to showcase data-driven fine
          motor skills.
          My path to robotics started unconventionally—I majored in Economics before diving into AI, with internships at
          Meta AI, Microsoft Research, and Google Search along the way. I’m driven by curiosity and currently I aim to
          design robot policies that master Robustness, Precision, and Dexterity.
          <br><br>
          <!-- In PhD program:
            Hi :wave: I am Liyiming Ke, 柯丽一鸣, or "Kay". I am a final-year grad student at University of Washington, advised by [Siddhartha Srinivasa](https://goodrobot.ai/). I research on Robotics :robot: Learning, with a focus on **Data-Driven Fine Manipulation**.
            As a test-bed for pushing the limit of fine manipulation, I built [a chopsticks robot](https://goodcherrybot.github.io/) to showcase the _precision_ and _dynamic reactivity_ of systems trained via reinforcement learning. I have also developed theories like [f-divergence framework for imitation learning and adversarial imitation learning](https://arxiv.org/abs/1905.12888) and [leveraging local-continuity in dynamics](https://arxiv.org/pdf/2310.12972).
            I was very fortunate to work with [Abhishek Gupta](https://homes.cs.washington.edu/~abhgupta/), [Tapomayukh Bhattacharjee](https://robotics.cornell.edu/faculty/tapomayukh-bhattacharjee-bio/), [Byron Boots](https://homes.cs.washington.edu/~bboots/) and [Sanjiban Choudary](https://sanjibanchoudhury.com/).
          -->
        <div id="more-bio" style="display: None">
          <br>
          <p>Liyiming Ke is a full stack robotist at Physical Intelligence researching on Machine Learning for Robot
            Manipulation. She earned her Ph.D. from the University of Washington with her thesis titled "Data-driven
            Fine Manipulation". She built a chopsticks-welding robot that demonstrate fine motor skills and developed
            theoretical frameworks for robot learning. She has led human-robot interactive demonstration at AAAS in 2020
            and has been selected as one of the Rising Stars in EECS 2023.</p>
        </div>
        <br>
        <a href="javascript:toggle_bio()">Formal Bio</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=EhOtO3cAAAAJ">G. Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/kelym">Github</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://www.linkedin.com/in/kelym/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://x.com/xkelym">Twitter</a>
        <br><br>
        kay at workplace dot company
        <br><br>
        </p>
      </div>
      <div id="intro-image">
        <img src="images/profile.jpg">
      </div>
    </div>

    <div id="filters" class="button-group">
      <!-- <button class="button" data-filter="*">Show All</button> -->
      <button class="button is-checked" data-filter=".highlight">Highlights</button>
      <button class="button" data-filter=".publication">Research</button>
      <button class="button" data-filter=".talk">Talks</button>
      <button class="button" data-filter=".misc">Misc</button>
    </div>

    <div class="grid">

      <!-- Highlights -->
      <div class="list-item highlight description" data-category="highlight">
        <!-- Some recent highlights from our research:-->
      </div>

      <!-- Preview Videos -->
      <div class="list-item highlight previews" data-category="highlight">

        <a href="https://goodcherrybot.github.io/"><video class="preview1" playsinline="" muted="" autoplay="" loop="">
            <source src="images/20250330-pi05-makebed.mp4" type="video/mp4">
          </video></a>

        <a href="https://goodcherrybot.github.io/"><video class="preview2" playsinline="" muted="" autoplay="" loop="">
            <source src="images/20250330-pi05-putsink.mp4" type="video/mp4">
          </video></a>

        <a href="https://goodcherrybot.github.io/"><video class="preview3" playsinline="" muted="" autoplay="" loop="">
            <source src="images/20250110-hirobot.mp4" type="video/mp4">
          </video></a>

      </div>
      <div class="list-item highlight previews" data-category="highlight">

        <a href="https://personalrobotics.github.io/CCIL/"><video class="preview1" playsinline="" muted="" autoplay=""
            loop="">
            <source src="images/20241101-ccil-applied.mp4" type="video/mp4">
          </video></a>

        <a href="https://www.physicalintelligence.company/blog/pi0"><video class="preview2" playsinline="" muted=""
            autoplay="" loop="">
            <source src="images/20241101-pizero_bussing_trashpile.mp4" type="video/mp4">
          </video></a>

        <a href="https://goodcherrybot.github.io/"><video class="preview3" playsinline="" muted="" autoplay="" loop="">
            <source src="images/20230315-cherrybot-granola_retry.mp4" type="video/mp4">
          </video></a>

      </div>

      <!-- Truncated Set of Highlights (Shown by Default) -->
      <div id="main-highlights">
        <!--
        <div class="list-item highlight" data-category="highlight">
          <b>some place</b> <a href="some link">some award</a>
        </div>
        -->
      </div>

      <!-- All Archived Highlights (Click to Show) -->
      <div id="more-highlights" style="display: None">

        <div class="list-item highlight" data-category="highlight">
          <p class="date">2024</p> <a href="https://personalrobotics.github.io/CCIL/">Can we improve robustness of
            Imitation Learning by generating synthetic corrective labels?</a>
        </div>

        <div class="list-item highlight" data-category="highlight">
          <p class="date">2024</p> <a href="https://www.physicalintelligence.company/blog/pi0">Can imitation learning
            benefits from large-scale pre-training on diverse task and robot embodiment?</a>
        </div>

        <div class="list-item highlight" data-category="highlight">
          <p class="date">2023</p> <a href="https://goodcherrybot.github.io/">Can we learn fine motor
            skills like picking up cherries with chopsticks using reinforcement learning?</a>
        </div>

        <div class="list-item highlight" data-category="highlight">
          <p class="date">2020</p> <a href="https://arxiv.org/abs/1905.12888">Viewing imitation learning from the frame
            of
            divergence minimization</a>
        </div>

        <!-- <div class="list-item highlight" data-category="highlight">
          <p class="date">Year</p> Stuff <a href="link">link</a>
        </div> -->

      </div>

      <!-- Toggle highlights button. -->
      <div class="list-item highlight toggle-button" data-category="highlight">
        <a id="toggle_highlights_button" href="javascript:toggle_highlights()">Show more</a>
      </div>




      <!-- Publications -->

      <div class="list-item publication" data-category="publication">
        <a href="https://www.pi.website/blog/pi05" class="thumbnail">
          <video playsinline="" muted="" autoplay="" loop="" width="180px">
            <source src="images/20250330-pi05-makebed.mp4" type="video/mp4">
          </video>
        </a>
        <div class="project-description">
          <h3><a href="https://www.pi.website/blog/pi05">&#960;0.5: A Vision-Language-Action
              Model with Open World Generalization</a></h3>
          <p>
            Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea
            Finn, Niccolo Fusai, Manuel Y Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon
            Jakubczak, Tim Jones, <b>Liyiming Ke</b>, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri,
            Suraj Nair, Karl Pertsch, Allen Z Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle
            Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, Ury Zhilinsky<br>
            <!--<i>CoRL 2025</i><br>-->
            <a href="https://www.physicalintelligence.company/download/pi05.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                We send mobile robots to many AirBnB houses to generalize tasks across diverse, real-world environments.
                Our robots can perform some household chores like cleaning kitchens in unseen houses.
              </span>
            </span>
          </p>
        </div>
      </div>


      <div class="list-item publication" data-category="publication">
        <a href="https://www.physicalintelligence.company/research/hirobot" class="thumbnail">
          <video playsinline="" muted="" autoplay="" loop="" width="180px">
            <source src="images/20250110-hirobot.mp4" type="video/mp4">
          </video>
        </a>
        <div class="project-description">
          <h3><a href="https://www.physicalintelligence.company/research/hirobot">Hi Robot: Open-Ended Instruction
              Following
              with Hierarchical
              Vision-Language-Action Models</a></h3>
          <p>
            Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, <b>Liyiming Ke</b>, Karl Pertsch, Quan Vuong, James Tanner,
            Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea
            Finn<br>
            <i>ICML 2025</i><br>
            <a href="https://arxiv.org/abs/2502.19417">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                We introduce a hierarchical system enabling robots to “think aloud” and deconstruct complex tasks ("make
                me a sandwich") into
                manageable steps ("pick up bread, pick up tomato, put tomato on the bread ..."). By combining a
                low-level action
                model for execution and a
                high-level
                vision-language model for reasoning and interaction with human inputs, we allow robots to follow complex
                instructions and perform tasks with high
                precision and adaptability.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://www.physicalintelligence.company/blog/pi0" class="thumbnail">
          <video playsinline="" muted="" autoplay="" loop="" width="180px">
            <source src="images/20241101-pizero.mp4" type="video/mp4">
          </video>
        </a>
        <div class="project-description">
          <h3><a href="https://www.physicalintelligence.company/blog/pi0">&#960;0: A Vision-Language-Action Flow
              Model for
              General Robot Control</a></h3>
          <p>
            Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai,
            Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, <b>Liyiming Ke</b>, Sergey Levine,
            Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong,
            Anna Walling, Haohuan Wang, Ury Zhilinsky<br>
            <i>RSS 2025</i><br>
            <a href="https://www.physicalintelligence.company/download/pi0.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Can you train cross-embodiment robotic policies over many many tasks and expect it to work? We show that
                it is promising: a big pre-training model can be finetuned on a single task and outperform
                dedicated policy that has only seen task-specific data.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://arxiv.org/abs/2410.20254" class="thumbnail">
          <img src="images/20241101-sim2real.jpg" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://arxiv.org/abs/2410.20254">Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn
              to Explore for Real-World RL</a></h3>
          <p>
            Andrew Wagenmaker, Kevin Huang, <b>Liyiming Ke</b>, Byron Boots, Kevin Jamieson, Abhishek Gupta<br>
            <i>NeurIPS 2024</i><br>
            <a href="https://arxiv.org/abs/2410.20254">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                We show that, learning an exploration policy in simulation can boost the real-world reinforcement
                learning
                finetuning efficiency (versus learning an optimal policy in the sim and transfer the policy).
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://arxiv.org/abs/2405.19307" class="thumbnail">
          <video playsinline="" muted="" autoplay="" loop="" width="180px">
            <source src="images/20241101-ccil-applied.mp4" type="video/mp4">
          </video>
        </a>
        <div class="project-description">
          <h3><a href="https://arxiv.org/abs/2405.19307">Data Efficient Behavior Cloning for Fine Manipulation via
              Continuity-based Corrective Labels</a></h3>
          <p>
            Abhay Deshpande, <b>Liyiming Ke</b>, Quinn Pfeifer, Abhishek Gupta, Siddhartha S. Srinivasa<br>
            <i>IROS 2024</i><br>
            <a href="https://personalrobotics.github.io/CCIL/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <a href="https://arxiv.org/abs/2405.19307">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                We apply CCIL to real world robotic manipulation tasks and it kinda worked after some design tweak. The
                most juice comes from setting up trust threshold for the generated labels in a task-agnostic way.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://arxiv.org/abs/2310.12972v1" class="thumbnail">
          <img src="images/20231019-ccil.png" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://arxiv.org/abs/2310.12972v1">CCIL: Continuity-based Data Augmentation for Corrective
              Imitation Learning</a></h3>
          <p>
            <b>Liyiming Ke*</b>, Yunchu Zhang*, Abhay Deshpande, Siddhartha Srinivasa, Abhishek Gupta<br>
            <i>ICLR 2024</i><br>
            <a href="https://personalrobotics.github.io/CCIL/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <a href="https://github.com/personalrobotics/CCIL">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <a href="https://arxiv.org/abs/2310.12972v1">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Enhances robustness of imitation learning by generating synthetic corrective labels:
                The trick is to leverage local continuity in the environment dynamics - and for regions that are
                discontinuous, quantify the confidence and skip them.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://goodcherrybot.github.io/" class="thumbnail">
          <video playsinline="" muted="" autoplay="" loop="" width="180px">
            <source src="images/20230315-cherry-picking.mp4" type="video/mp4">
          </video>
        </a>
        <div class="project-description">
          <h3><a href="https://goodcherrybot.github.io/">Cherry Picking with Reinforcement Learning</a></h3>
          <p>
            Yunchu Zhang*, <b>Liyiming Ke*</b>, Abhay Deshpande, Abhishek Gupta, Siddhartha Srinivasa<br>
            <i>RSS 2023</i><br>
            <a href="https://goodcherrybot.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <a href="https://arxiv.org/abs/2303.05508">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Use reinforcement learning to learn fine motor skills: pick up slippery cherries with chopsticks under
                wind or human disturbances. And I refuse to do parameter sweeping or random seed cherry picking.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://sites.google.com/view/real-orl" class="thumbnail">
          <img src="images/20220930-real-offlinerl.png" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://sites.google.com/view/real-orl">Real World Offline Reinforcement Learning with Realistic
              Data Sources</a></h3>
          <p>
            Gaoyue Zhou*, <b>Liyiming Ke*</b>, Siddhartha Srinivasa, Abhinav Gupta, Aravind Rajeswaran, Vikash Kumar<br>
            <i>ICRA 2023</i><br>
            <a href="https://sites.google.com/view/real-orl">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <a href="https://arxiv.org/abs/2210.06479">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Eval offline RL in real-world: emphasize on data being "kinda good" but not perfect.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://personalrobotics.cs.washington.edu/publications/ke2021grasping.pdf" class="thumbnail">
          <img src="images/20201101-chopsticks-grasping.jpg" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://personalrobotics.cs.washington.edu/publications/ke2021grasping.pdf">Grasping with
              Chopsticks: Combating Covariate Shift in Model-free Imitation Learning for Fine Manipulation</a></h3>
          <p>
            <b>Liyiming Ke</b>, Jingqiang Wang, Tapomayukh Bhattacharjee, Byron Boots, Siddhartha S. Srinivasa<br>
            <i>ICRA 2021</i><br>
            <a href="https://arxiv.org/abs/2011.06719">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Teach a robot to use chopsticks for precise manipulation tasks through human demonstrations: Addresses
                covariate shift in imitation learning by noise-injection, object-centric transformation and
                bunch of hacks.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://personalrobotics.cs.washington.edu/publications/ke2020teleop.pdf" class="thumbnail">
          <img src="images/20200630-chopsticks-teleop.gif" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://personalrobotics.cs.washington.edu/publications/ke2020teleop.pdf">Telemanipulation with
              Chopsticks: Analyzing Human Factors in User Demonstrations</a></h3>
          <p>
            <b>Liyiming Ke</b>, Ajinkya Kamat, Jingqiang Wang, Tapomayukh Bhattacharjee, Christoforos Mavrogiannis,
            Siddhartha S. Srinivasa<br>
            <i>IROS 2020</i><br>
            <a href="https://arxiv.org/abs/2008.00101">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Built a chopsticks robot and a fun human-interactive demo collection interface: turns out that tracking
                a
                wand and commmand the robot can be really easy.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://arxiv.org/abs/1905.12888" class="thumbnail">
          <img src="images/20190608-fimitation-teaser.png" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://arxiv.org/abs/1905.12888">Imitation Learning as f-Divergence Minimization</a></h3>
          <p>
            <b>Liyiming Ke</b>, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, Siddhartha Srinivasa<br>
            <i>WAFR 2020</i><br>
            <a href="https://arxiv.org/abs/1905.12888">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                A unified theoretical framework for imitation learning! Turns out some SOTA algorithms are using
                f-divergence. We show how different divergence measures lead to different imitation learning approaches.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html"
          class="thumbnail">
          <img src="images/20190607-vln-teaser.png" alt="" />
        </a>
        <div class="project-description">
          <h3><a
              href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ke_Tactical_Rewind_Self-Correction_via_Backtracking_in_Vision-And-Language_Navigation_CVPR_2019_paper.html">Tactical
              Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a></h3>
          <p>
            <b>Liyiming Ke</b>, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi,
            Siddhartha Srinivasa<br>
            <i>CVPR 2019</i>
            <font color="49bf9"><i>&#9733; Oral Presentation, CVPR (5.6%) &#9733;</i></font><br>
            <a href="https://arxiv.org/abs/1903.02547">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Baking Search and Planning into ML-based navigation: We propose a new framework for VL navigation,
                enabling agents to recover from mistakes by maintaining internal search tree and returning to previous
                positions and trying alternative
                paths.
              </span>
            </span>
          </p>
        </div>
      </div>

      <div class="list-item publication" data-category="publication">
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/10061" class="thumbnail">
          <img src="images/20160101-email-teaser.png" alt="" />
        </a>
        <div class="project-description">
          <h3><a href="https://ojs.aaai.org/index.php/AAAI/article/view/10061">Behavioral Experiments in Email Filter
              Evasion</a></h3>
          <p>
            <b>Liyiming Ke</b>, Bo Li, Yevgeniy Vorobeychik<br>
            <i>AAAI 2016</i><br>
            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/10061">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
            <span class="excerpt-container">
              <a href="#" onclick="event.preventDefault();">Summary</a>
              <span class="excerpt-content">
                Studies how humans attempt to evade email spam filters.
                Provides insights into adversarial behavior and implications for security system design.
              </span>
            </span>
          </p>
        </div>
      </div>

      <!-- Talks -->
      <!-- <div class="list-item talk description" data-category="talk">
          Some of my slides can be found <a href="https://slides.com/andyzeng">here</a>
        </div> -->

      <div class="list-item talk" data-category="talk">
        <p class="date">2025</p>RSS Workshop (upcoming)
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>Actuate (upcoming)
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date">2024</p>OpenAI Reading Group
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>University of Washington, Robotics Seminar (<a
          href="https://www.youtube.com/watch?v=LeHYQVR8a8k">video</a>)
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date">2023</p>Stanford University, <a href="https://iliad.stanford.edu/">ILIAD Lab</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>University of California Berkeley, <a href="https://bair.berkeley.edu/">BAIR Lab</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>Carnegie Mellon University, <a href="https://www.cs.cmu.edu/~cga/">Atkeson Lab</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>Shanghai Jiaotong University, <a href="https://automation.sjtu.edu.cn/">Department of
          Automation</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p>Stanford University, <a href="https://iprl.stanford.edu/">Interactive Perception and
          Robot
          Learning Lab</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date">2022</p>Cornell University, <a href="https://emprise.cs.cornell.edu/">EmPRISE
          Lab</a>
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date"></p> <a href="https://mila.quebec/en/">Mila - Quebec AI Institute</a> (<a
          href="https://www.youtube.com/watch?v=LeHYQVR8a8k">video</a>)
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date">2021</p>MetaAI Reading Group
      </div>

      <div class="list-item talk" data-category="talk">
        <p class="date">2018</p>Microsoft Research Dialogue Group Reading Group
      </div>


      <!-- Services -->


      <div class="list-item misc" data-category="misc">
        <p class="date"> </p>Reviewer of RSS, CoRL, ICLR, NeurIPS, ICRA, IJRR, IROS, RA-L, HRI, AAMAS, IJCAI
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2025</p>We <a href="https://github.com/Physical-Intelligence/openpi">open source
          &#960; 0 on Github</a> to empower the community by sharing our foundation models.
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2024</p>Our first generalist robotic model, &#960; 0,
        is featured on <a href="https://www.nytimes.com/2024/11/04/business/dealbook/selzer-poll-trump-trade.html">
          New York Times</a> and <a href="https://www.wired.com/story/physical-intelligence-home-robot/"> Wired</a>.
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2023</p>Honored to be selected as one of the <a
          href="https://www.eecs.mit.edu/community-equity/rising-stars-in-eecs/">Rising Stars in EECS</a>
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2020</p>Chopsticks Robot featured on <a
          href="https://spectrum.ieee.org/video-friday-agility-robotics-robot-production">IEEE Spectrum Video
          Friday</a>
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2020</p>Led a human-robot interactive demo at the <a
          href="https://www.aaas.org/events/2020-aaas-annual-meeting">AAAS
          gathering</a>
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2017</p>Graduated as one of the <a href="https://my.vanderbilt.edu/collegescholars/">Honor
          Scholars</a> from Vanderbilt University
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date">2015</p>First prize in the Vanderbilt Student Consulting for Non-profit Organization
      </div>

      <div class="list-item misc" data-category="misc">
        <p class="date"> - </p>Inspired by
        <ul
          style="margin-top: 5px; margin-bottom: 0px; margin-left: 0px; list-style-type: none; padding-left: 0; display: inline;">
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://distill.pub/">Distill</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://lilianweng.github.io/">Lil' Log</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://colah.github.io/">Colah's Blog</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <!--
          <li style="display: inline;"><a href="http://neuralnetworksanddeeplearning.com/">Michael Nielsen</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://karpathy.github.io/">Andrej Karpathy</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://danielseita.github.io/">Seita's Place</a></li>
          <li style="display: inline;">&nbsp;&bull;&nbsp;</li>
          <li style="display: inline;"><a href="https://www.ruder.io/">Sebastian Ruder</a></li>
          -->
        </ul>
      </div>

    </div>
    <div id="footer">
      Website template by <a href="https://andyzeng.github.io/">Andy Zeng</a> and <a
        href="https://jonbarron.info/">Jon's
        website</a>.
    </div>

  </div>

  <script>

    // Isotope grid.
    var $grid = $('.grid').isotope({
      itemSelector: '.list-item',
      layoutMode: 'fitRows',
      transitionDuration: 0,
      stagger: 10,
      initLayout: false,
      getSortData: {
        name: '.name',
        symbol: '.symbol',
        number: '.number parseInt',
        category: '[data-category]',
        weight: function (itemElem) {
          var weight = $(itemElem).find('.weight').text();
          return parseFloat(weight.replace(/[\(\)]/g, ''));
        }
      }
    });

    // Bind filter button click.
    $('#filters').on('click', 'button', function () {
      var filterValue = $(this).attr('data-filter');
      localStorage.setItem('filterValue', filterValue);
      $grid.isotope({ filter: filterValue });
    });

    // Change is-checked class on buttons.
    $('.button-group').each(function (i, buttonGroup) {
      var $buttonGroup = $(buttonGroup);
      $buttonGroup.on('click', 'button', function () {
        $buttonGroup.find('.is-checked').removeClass('is-checked');
        $(this).addClass('is-checked');
      });
    });

    function update_isotope() {
      // Retrieve cached button click.
      var defaultFilterValue = localStorage.getItem('filterValue');
      if (defaultFilterValue == null) {
        defaultFilterValue = ".highlight"
      }
      $grid.isotope({ filter: defaultFilterValue });
      var buttons = document.getElementsByClassName("button");
      for (var currButton of buttons) {
        if (currButton.getAttribute('data-filter') == defaultFilterValue) {
          currButton.classList.add('is-checked');
        } else {
          currButton.classList.remove('is-checked');
        }
      }
    }

    function toggle_bio() {
      var x = document.getElementById("more-bio");
      if (x.style.display === "none") {
        x.style.display = "block";
      } else {
        x.style.display = "none";
      }
    }

    function toggle_highlights() {
      var x = document.getElementById("main-highlights");
      var y = document.getElementById("more-highlights");
      var b = document.getElementById("toggle_highlights_button")
      if (y.style.display === "none") {
        x.style.display = "none";
        y.style.display = "block";
        b.innerHTML = "Show less"
        update_isotope();
      } else {
        x.style.display = "block";
        y.style.display = "none";
        b.innerHTML = "Show more"
        update_isotope();
      }
    }

    update_isotope();

  </script>
</body>

</html>